{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,TensorDataset,random_split,Dataset,ConcatDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorizationModel, self).__init__()\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(16, 1 , kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationModel_a(nn.Module):\n",
    "    def __init__(self ):#,  encoder):\n",
    "        super(ColorizationModel_a, self).__init__()\n",
    "        # Use the provided encoder as the encoder of the new model\n",
    "        # self.encoder = encoder\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(16, 3 , kernel_size=3, stride=1, padding=1),  # Output has 3 channels\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        # x = self.encoder(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationModel_b(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorizationModel_b, self).__init__()\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(16, 3 , kernel_size=3, stride=1, padding=1),  # Output has 3 channels\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Decoder\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorCombinationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorCombinationModel, self).__init__()\n",
    "        # Define convolutional layers to process input channels\n",
    "        self.conv1 = nn.Conv2d(in_channels=8, out_channels=30, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=30, out_channels=50, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=50, out_channels=1, kernel_size=3, padding=1)\n",
    "        # Define activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_colorization_model_a():\n",
    "    return ColorizationModel_a()\n",
    "def create_colorization_model_b():\n",
    "    return ColorizationModel_b()\n",
    "def create_ColorCombinationModel():\n",
    "    return ColorCombinationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    # Fixed paths to model state dictionaries\n",
    "    encoder_path = 'model_encoder_trained.pth'\n",
    "    model_paths_a =['model_1.pth', 'model_2.pth', 'model_3.pth', 'model_4.pth']\n",
    "    model_paths_b =['model_5.pth', 'model_6.pth', 'model_7.pth', 'model_8.pth'] \n",
    "    hue_path = 'model_Hue.pth'\n",
    "    saturation_path = 'model_Saturation.pth'\n",
    "    value_path = 'model_Value.pth'\n",
    "    \n",
    "    Auto_encoder = ColorizationModel()\n",
    "    encoder = Auto_encoder.encoder\n",
    "    encoder.load_state_dict(torch.load(encoder_path))\n",
    "\n",
    "    # Load other models\n",
    "    models = []\n",
    "    for path in model_paths_a:\n",
    "        model = create_colorization_model_a()\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        models.append(model)\n",
    "    for path in model_paths_b:\n",
    "        model = create_colorization_model_b()\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        models.append(model)\n",
    "    # Load hue model\n",
    "    hue_model = create_ColorCombinationModel()\n",
    "    hue_model.load_state_dict(torch.load(hue_path))\n",
    "\n",
    "    # Load saturation model\n",
    "    saturation_model = create_ColorCombinationModel()\n",
    "    saturation_model.load_state_dict(torch.load(saturation_path))\n",
    "\n",
    "    # Load value model\n",
    "    value_model = create_ColorCombinationModel()\n",
    "    value_model.load_state_dict(torch.load(value_path))\n",
    "\n",
    "    return encoder, models, hue_model, saturation_model, value_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_models(model_encoder, models, model_Hue, model_Saturation, model_Value, grayscale_image):\n",
    "    # Pass the grayscale image through the encoder\n",
    "    encoded_image = model_encoder(grayscale_image)\n",
    "    \n",
    "    # Initialize lists to store outputs from individual models\n",
    "    hue_outputs = []\n",
    "    saturation_outputs = []\n",
    "    value_outputs = []\n",
    "    \n",
    "    # Iterate through each model and generate HSV outputs\n",
    "    for model in models:\n",
    "        hsv_output = model(encoded_image)\n",
    "        \n",
    "        # Split HSV output into individual channels\n",
    "        hue_channel = hsv_output[:, 0:1, :, :]\n",
    "        saturation_channel = hsv_output[:, 1:2, :, :]\n",
    "        value_channel = hsv_output[:, 2:3, :, :]\n",
    "        \n",
    "        # Append each channel to respective lists\n",
    "        hue_outputs.append(hue_channel)\n",
    "        saturation_outputs.append(saturation_channel)\n",
    "        value_outputs.append(value_channel)\n",
    "    \n",
    "    # Combine the outputs from all models\n",
    "    combined_hue_output = torch.cat(hue_outputs, dim=1)\n",
    "    combined_saturation_output = torch.cat(saturation_outputs, dim=1)\n",
    "    combined_value_output = torch.cat(value_outputs, dim=1)\n",
    "    \n",
    "    # Combine the outputs to create HSV images\n",
    "    colorized_image_hue = model_Hue(combined_hue_output)\n",
    "    colorized_image_saturation = model_Saturation(combined_saturation_output)\n",
    "    colorized_image_value = model_Value(combined_value_output)\n",
    "    \n",
    "    # Combine the colorized images into one\n",
    "    colorized_image = torch.cat([colorized_image_hue, colorized_image_saturation, colorized_image_value], dim=1)\n",
    "    \n",
    "    return colorized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_color_enhancement(models, best_model_red, best_model_green, best_model_blue, grayscale_patches):\n",
    "    red_channels   = []\n",
    "    green_channels = []\n",
    "    blue_channels  = []\n",
    "    \n",
    "    for patch in grayscale_patches:\n",
    "        patch = patch.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "        for model in models:\n",
    "            color_prediction = model(patch)\n",
    "            red_channels.append(color_prediction[0, 0]) \n",
    "            green_channels.append(color_prediction[0, 1]) \n",
    "            blue_channels.append(color_prediction[0, 2]) \n",
    "\n",
    "    red_channels_tensor   = torch.stack(red_channels,   dim=0).unsqueeze(0)\n",
    "    green_channels_tensor = torch.stack(green_channels, dim=0).unsqueeze(0)\n",
    "    blue_channels_tensor  = torch.stack(blue_channels,  dim=0).unsqueeze(0)\n",
    "\n",
    "    pred_red   = best_model_red(red_channels_tensor)\n",
    "    pred_green = best_model_green(green_channels_tensor)\n",
    "    pred_blue  = best_model_blue(blue_channels_tensor)\n",
    "\n",
    "    stacked_channels = torch.cat([pred_red, pred_green, pred_blue], dim=1)\n",
    "\n",
    "    # Create a placeholder for the final reconstructed image\n",
    "    final_image = torch.zeros_like(stacked_channels)\n",
    "\n",
    "    # Calculate the number of patches along each dimension\n",
    "    num_patches = len(grayscale_patches)\n",
    "    rows = int(np.sqrt(num_patches))\n",
    "    cols = (num_patches + rows - 1) // rows\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    for i, patch in enumerate(stacked_channels.split(1)):\n",
    "        reshaped_patch = patch.squeeze(0).permute(1, 2, 0).detach().cpu().numpy()\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.imshow(reshaped_patch)\n",
    "        plt.title(f'Patch {i+1}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Add the patch to the final image\n",
    "        row_idx = i // cols\n",
    "        col_idx = i % cols\n",
    "        final_image[:, :, row_idx * 128:(row_idx + 1) * 128, col_idx * 128:(col_idx + 1) * 128] = patch\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return final_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_channel(image_path):\n",
    "    # Read the image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(\"Error: Unable to read image.\")\n",
    "        return None\n",
    "    # Check number of channels\n",
    "    num_channels = img.shape[2]\n",
    "    if num_channels == 1:\n",
    "        print(\"Image already has one channel.\")\n",
    "        return img\n",
    "    elif num_channels == 3:\n",
    "        # Convert to grayscale\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)-0.5\n",
    "        print(\"Converted image to one channel.\")\n",
    "        return gray_img\n",
    "    else:\n",
    "        print(\"Unsupported number of channels.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image_into_squares(image):\n",
    "    squares = []\n",
    "    locations = []\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "    square_size = 128\n",
    "\n",
    "    for y in range(0, height, square_size):\n",
    "        for x in range(0, width, square_size):\n",
    "            square = image[y:y+square_size, x:x+square_size]\n",
    "            # Adjust square size if it's smaller than expected\n",
    "            square_height, square_width = square.shape\n",
    "            if square_height < square_size or square_width < square_size:\n",
    "                square = cv2.resize(square, (square_size, square_size))\n",
    "            squares.append(square.reshape(1, square_size, square_size))  # Reshape the square\n",
    "            locations.append((x, y))\n",
    "\n",
    "    # Convert squares list to a tensor\n",
    "    squares_tensor = torch.tensor(squares, dtype=torch.float)\n",
    "\n",
    "    return squares_tensor, locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model, models, hue_model, saturation_model, value_model = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"2.jpg\"\n",
    "\n",
    "result_image = convert_to_one_channel(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares, locations = split_image_into_squares(result_image)\n",
    "img_plot = combine_models(encoder_model, models, hue_model, saturation_model, value_model, squares)+0.5\n",
    "max_x = max(location[0] for location in locations) + 128  # Get the maximum x coordinate\n",
    "max_y = max(location[1] for location in locations) + 128  # Get the maximum y coordinate\n",
    "output_tensor_resized = F.interpolate(img_plot, size=(max_y, max_x), mode='nearest')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))  # Create a figure with two subplots\n",
    "\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "processed_image_tensor = output_tensor_resized[0]  # Assuming you want to display the first image in the batch\n",
    "\n",
    "# Convert HSV image tensor to numpy array and scale values to [0, 255]\n",
    "processed_image_np = processed_image_tensor.permute(1, 2, 0).detach().cpu().numpy() * 255\n",
    "processed_image_np = processed_image_np.astype(np.uint8)\n",
    "\n",
    "# Convert HSV to RGB\n",
    "processed_image_rgb = cv2.cvtColor(processed_image_np, cv2.COLOR_HSV2RGB)\n",
    "axes[1].imshow(processed_image_rgb)\n",
    "axes[1].set_title('Processed Image')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
